---

# This is a pre-populated all.yml. 
# Fill in the needed info to set up your cluster. 
# If you have any questions, contact RPC-STORAGE

# Enable the Ubuntu 18.04 default time synchronization
# service.
ntp_service_enabled: true
ntp_daemon_type: chronyd

copy_admin_key: true

ceph_release_num:
  dumpling: 0.67
  emperor: 0.72
  firefly: 0.80
  giant: 0.87
  hammer: 0.94
  infernalis: 9
  jewel: 10
  kraken: 11
  luminous: 12
  mimic: 13
  nautilus: 14
  octopus: 15
  pacific: 16
  quincy: 17
  reef: 18
  dev: 99


# Inventory host group variables
mon_group_name: mons
osd_group_name: osds
rgw_group_name: rgws
mds_group_name: mdss
nfs_group_name: nfss
mgr_group_name: mgrs
#restapi_group_name: restapis
#rbdmirror_group_name: rbdmirrors
#client_group_name: clients
#iscsi_gw_group_name: iscsigws



# Pull ceph from upstream

ceph_origin: repository
ceph_repository: community
ceph_mirror: http://download.ceph.com
ceph_stable_key: https://download.ceph.com/keys/release.asc
ceph_stable_release: octopus
ceph_stable_repo: "{{ ceph_mirror }}/debian-{{ ceph_stable_release }}"

# Pull NFS-Ganesha 2.7 if using CephfS (Uncomment only if using CephFS)
#nfs_ganesha_stable: true # use stable repos for nfs-ganesha
#nfs_ganesha_stable_branch: V2.7-stable
#nfs_ganesha_stable_deb_repo: "{{ ceph_mirror }}/nfs-ganesha/deb-{{ nfs_ganesha_stable_branch }}/{{ ceph_stable_release }}"


# Configure Frontend and Backend network ranges

monitor_address_block: FRONTEND-NETWORK-CIDR
public_network: FRONTEND-NETWORK-CIDR
cluster_network: BACKEND-NETWORK-CIDR

# osd backend to use. This will be bluestore until further notice
osd_objectstore: bluestore



# CephFS backend (only uncomment if setting up CephFS)

# Set this to true to enable File access via NFS.  Requires an MDS role.
#nfs_file_gw: true
# Set this to true to enable Object access via NFS. Requires an RGW role.
#nfs_obj_gw: false   # DO NOT EVER SET THIS TO TRUE


# Required for CRUSH hierarchy { osd_crush_location: { root: default, rack: rack1, host: ceph01 } }
create_crush_tree: True

# Default autoscale mode to True
pg_autoscale_mode: True


# Openstack Pools
openstack_config: True
openstack_glance_pool:
  name: "images"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  target_size_ratio: 0.2
openstack_cinder_pool:
  name: "volumes"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  target_size_ratio: 1.0
openstack_nova_pool:
  name: "vms"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  target_size_ratio: 0.2
openstack_cinder_backup_pool:
  name: "backups"
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
  size: "{{ osd_pool_default_size }}"
  min_size: "{{ osd_pool_default_min_size }}"
  target_size_ratio: 0.2

openstack_pools:
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_nova_pool }}"
  - "{{ openstack_cinder_backup_pool }}"

openstack_keys:
  - { name: client.glance, caps: { mon: "profile rbd", osd: "allow class-read object_prefix rbd_children, profile rbd pool=volumes, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "allow class-read object_prefix rbd_children, profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder-backup, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_backup_pool.name }}"}, mode: "0600" }

# CephFS pools (only uncomment if setting up CephFS)

#cephfs: cephfs # name of the ceph filesystem
#cephfs_data: cephfs_data # name of the data pool for a given filesystem
#cephfs_metadata: cephfs_metadata # name of the metadata pool for a given filesystem

#cephfs_data_pool:
#  name: "{{ cephfs_data }}"
#  pgs: "{{ osd_pool_default_pg_num }}"
#  size: "{{ osd_pool_default_size }}"
#  min_size: "{{ osd_pool_default_min_size }}"
#  rule_name: "replicated_rule"

#cephfs_metadata_pool:
#  name: "{{ cephfs_metadata }}"
#  pgs: "{{ osd_pool_default_pg_num }}"
#  size: "{{ osd_pool_default_size }}"
#  min_size: "{{ osd_pool_default_min_size }}"
#  rule_name: "replicated_rule"


#cephfs_pools:
#  - "{{ cephfs_data_pool }}"
#  - "{{ cephfs_metadata_pool }}"

dashboard_admin_password: <PASSWORD>
grafana_admin_password: <DIFFERENT_PASSWORD>

# Tuning Defaults DO NOT CHANGE WITHOUT CONSULTING RPC-STORAGE

osd_pool_default_size: 3
osd_pool_default_min_size: 2
osd_pool_default_pg_num: 32
osd_pool_default_pgp_num: 32

ceph_conf_overrides: 
   "global":
     "debug_lockdep": "0/0"
     "debug_context": "0/0"
     "debug_crush": "0/0"
     "debug_buffer": "0/0"
     "debug_timer": "0/0"
     "debug_filer": "0/0"
     "debug_objecter": "0/0"
     "debug_rados": "0/0"
     "debug_rbd": "0/0"
     "debug_journaler": "0/0"
     "debug_objectcacher": "0/0"
     "debug_client": "0/0"
     "debug_osd": "0/0"
     "debug_optracker": "0/0"
     "debug_objclass": "0/0"
     "debug_filestore": "0/0"
     "debug_bluestore": "0/0"
     "debug_journal": "0/0"
     "debug_ms": "0/0"
     "debug_tp": "0/0"
     "debug_auth": "0/0"
     "debug_finisher": "0/0"
     "debug_heartbeatmap": "0/0"
     "debug_perfcounter": "0/0"
     "debug_asok": "0/0"
     "debug_throttle": "0/0"
     "debug_paxos": "0/0"
     "debug_rgw": "0/0"
     "debug_monc": "0/0"
     "debug_mon": "0/0"
     "max_open_files": "26234859"
     "osd_pool_default_pg_num": "{{ osd_pool_default_pg_num }}"
     "osd_pool_default_pgp_num": "{{ osd_pool_default_pgp_num }}"
     "osd_deep_scrub_interval": "1209600"
     "mon_osd_down_out_interval": "900"
     "mon_pg_warn_max_object_skew": "0"
     "objecter_inflight_op_bytes": 524288000
     "objecter_inflight_ops": 5120
     "mon_warn_on_pool_pg_num_not_power_of_two": "false"
     "bluestore_warn_on_bluefs_spillover": "false"     
     "mon_warn_pg_not_deep_scrubbed_ratio": 0
     "mon_warn_pg_not_scrubbed_ratio": 0
     "auth_allow_insecure_global_id_reclaim": "false"
     "mon_target_pg_per_osd": 100
   "osd":
     "osd_heartbeat_min_size": "9000"
     "osd_snap_trim_priority": "1"
     "osd_snap_trim_sleep": "0.1"
     "osd_pg_max_concurrent_snap_trims": "1"
     "osd_scrub_sleep": "0.1"
     "osd_scrub_priority": "1"
     "osd_scrub_chunk_min": "1"
     "osd_scrub_chunk_max": "5"
     "osd_scrub_load_threshold": "10.0"
     "osd_scrub_min_interval": "129600"
   "client":
     "rgw cache lru size": "250000"
     "rgw thread pool size": "8192"
     "rgw num rados handles": "8"
     "rbd_cache": "True"
     "rbd_cache_max_dirty": 50331648
     "rbd_cache_max_dirty_age": 15
     "rbd_cache_size": 67108864
     "rbd_cache_target_dirty": 33554432
     "rbd_cache_writethrough_until_flush": "False"
## UNCOMMENT FOR RGW
#   "client.rgw.{{ hostvars[inventory_hostname]['ansible_facts']['hostname'] }}.rgw0":
#     "rgw keystone api version": "3"
#     "rgw keystone url": "<INTERNAL KEYSTONE ENDPOINT>"
#     "rgw keystone admin user": "swift"
#     "rgw keystone admin password": "<PASSWORD FROM OPENSTACK>"
#     "rgw keystone admin tenant": "service"
#     "rgw keystone admin domain": "default"
#     "rgw keystone accepted roles": "Member, _member_, admin"
#     "rgw keystone token cache size:": "10000"
#     "rgw keystone revocation interval": "900"
#     "rgw s3 auth use keystone": "true"
#     "rgw swift account in url": "true"
#     "rgw keystone implicit tenants": "true"

disable_transparent_hugepage: true
os_tuning_params:
  - { name: kernel.pid_max, value: 4194303 }
  - { name: fs.file-max, value: 262144 }
  - { name: vm.vfs_cache_pressure, value: 20 }
  - { name: vm.dirty_background_ratio, value: 3}
  - { name: vm.dirty_ratio, value: 10 }
  - { name: vm.swappiness, value: 0 }
  - { name: net.ipv4.tcp_slow_start_after_idle, value: 0 }
  - { name: net.ipv4.tcp_max_syn_backlog, value: 4096 }
  - { name: net.core.rmem_max, value: 56623104 }
  - { name: net.core.wmem_max, value: 56623104 }
  - { name: net.core.rmem_default, value: 56623104 }
  - { name: net.core.wmem_default, value: 56623104 }
  - { name: net.core.optmem_max, value: 40960 }
  - { name: net.ipv4.tcp_rmem, value: 4096 87380 56623104 }
  - { name: net.ipv4.tcp_wmem, value: 4096 87380 56623104 }
  - { name: net.netfilter.nf_conntrack_max, value: 1048576 }
  - { name: net.core.somaxconn, value: 8192 }
